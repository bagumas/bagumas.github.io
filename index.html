<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Samuel Baguma — AI Security</title>
  <meta name="description" content="Samuel Baguma — AI Security Engineer. Securing ML models, inference APIs, and data privacy.">
  <link rel="icon" href="/assets/favicon.ico">
  <link rel="stylesheet" href="/style.css">
  <meta property="og:title" content="Samuel Baguma — AI Security">
  <meta property="og:description" content="Portfolio, projects, and blog on securing AI systems.">
  <meta property="og:type" content="website">
</head>
<body>
  <header class="site-header">
    <div class="wrap">
      <h1 class="brand">Samuel Baguma</h1>
      <nav>
        <a href="/">About</a>
        <a href="/projects.html">Projects</a>
        <a href="/blog/">Blog</a>
        <a href="https://github.com/bagumas" target="_blank" rel="noopener">GitHub</a>
        <a href="https://linkedin.com/in/samuel-baguma-37a45110" target="_blank" rel="noopener">LinkedIn</a>
      </nav>
    </div>
  </header>

  <main class="wrap">
    <section class="hero">
      <h2>AI Security Engineer</h2>
      <p class="subtitle">I secure machine learning systems end‑to‑end — models, inference APIs, data pipelines, and privacy.</p>
      <div class="cta-row">
        <a class="btn" href="/projects.html">View Projects</a>
        <a class="btn secondary" href="/resume.pdf">Download Résumé</a>
      </div>
    </section>

    <section class="card">
      <h3>About</h3>
      <p>
        I specialize in <strong>securing AI in production</strong>: model encryption at rest, authenticated inference, data privacy, and defenses
        against adversarial attacks (poisoning, evasion, extraction, inversion, and prompt injection). I enjoy building
        <em>defense‑in‑depth</em> architectures and reproducible security tests that teams can run in CI.
      </p>
      <p>
        Recent work: encrypted TensorFlow models with AES‑GCM; API‑key gated inference; OAuth‑protected web app; and adversarial test harnesses
        using ART/TextAttack. I write about practical ML security, governance, and privacy on my blog.
      </p>
    </section>

    <section class="grid">
      <a class="tile" href="/projects.html#smcp">
        <h4>Secure Model Context Protocol</h4>
        <p>Trust boundaries & signed context for LLMs and inference APIs.</p>
      </a>
      <a class="tile" href="/projects.html#ai-defense-pipeline">
        <h4>AI Defense Pipeline</h4>
        <p>Adversarial detection & automated mitigation in CI/CD.</p>
      </a>
      <a class="tile" href="/projects.html#healthbot-security">
        <h4>HealthBot AI Security</h4>
        <p>Privacy‑preserving clinical chatbot with LLM guardrails.</p>
      </a>
    </section>
  </main>

  <footer class="site-footer">
    <div class="wrap">
      <p>© 2025 Samuel Baguma • <a href="https://github.com/bagumas" target="_blank" rel="noopener">GitHub</a> • <a href="https://linkedin.com/in/samuel-baguma-37a45110" target="_blank" rel="noopener">LinkedIn</a></p>
    </div>
  </footer>
</body>
</html>
