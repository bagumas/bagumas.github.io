# üí¨ Adversarial Prompt Attacks ‚Äî LLM Security

Prompt-based attacks manipulate LLMs to break rules or leak sensitive information.

## ‚ö†Ô∏è Attack Types
- Prompt Injection / Jailbreaks.
- Indirect Injection via web content.
- Impersonation & prompt override.

## üõ°Ô∏è Defenses
- Input sanitization & validation.
- Context-bounded prompts.
- Moderation APIs (OpenAI, Hugging Face).
- Guardrails frameworks (e.g., NeMo, WhyLabs).

**Bottom line:** Secure both *inputs and outputs* to harden LLM applications.

---
#LLMSecurity #PromptInjection #AISecurity
