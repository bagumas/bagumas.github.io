# ğŸ­ Evasion Attacks â€” Fooling Deployed Models

Adversaries craft perturbed inputs to deceive models during inference â€” causing misclassifications while appearing normal to humans.

## âš¡ Common Techniques
- **FGSM:** single-step perturbation.
- **PGD:** iterative refinement of perturbations.
- **C&W:** minimal, stealthy adversarial noise.
- **Text-based attacks:** NLP manipulation via TextAttack.

## ğŸ›¡ï¸ Defenses
- Adversarial training with perturbed examples.
- Input preprocessing (resize, crop, normalize).
- Gradient masking with Gaussian noise.

**Bottom line:** Continuous adversarial testing is critical for deployed AI systems.

---
#AISecurity #EvasionAttacks #AdversarialML
