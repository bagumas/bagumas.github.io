# â˜ ï¸ Poisoning Attacks â€” When Hackers Train Your AI

Attackers can compromise AI models during training â€” not just deployment. These attacks manipulate training data to insert backdoors, bias, or sabotage model behavior.

## ğŸ¯ Why Attackers Poison Models
- Induce bias or misclassifications.
- Insert hidden triggers (backdoors).
- Enable evasion and fraud.

## ğŸ§  Types of Attacks
- **Label flipping:** simple data manipulation.
- **Backdoor poisoning:** trigger-based misclassification.
- **Clean-label attacks:** stealthy manipulations with valid labels.

## ğŸ›¡ï¸ Defenses
- Strict data access control.
- Dataset encryption & versioning.
- Integrity validation via SHA-256.
- Adversarial testing using ART & TextAttack.

**Bottom line:** Protect your *training pipelines* as tightly as your models.

---
#AISecurity #PoisoningAttacks #MLOps #AdversarialML
