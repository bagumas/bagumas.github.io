# ☠️ Poisoning Attacks — When Hackers Train Your AI

Attackers can compromise AI models during training — not just deployment. These attacks manipulate training data to insert backdoors, bias, or sabotage model behavior.

## 🎯 Why Attackers Poison Models
- Induce bias or misclassifications.
- Insert hidden triggers (backdoors).
- Enable evasion and fraud.

## 🧠 Types of Attacks
- **Label flipping:** simple data manipulation.
- **Backdoor poisoning:** trigger-based misclassification.
- **Clean-label attacks:** stealthy manipulations with valid labels.

## 🛡️ Defenses
- Strict data access control.
- Dataset encryption & versioning.
- Integrity validation via SHA-256.
- Adversarial testing using ART & TextAttack.

**Bottom line:** Protect your *training pipelines* as tightly as your models.

---
#AISecurity #PoisoningAttacks #MLOps #AdversarialML
