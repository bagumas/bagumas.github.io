---
layout: post
title: "ğŸ” #3: Evasion Attacks â€” Fooling Deployed AI Models"
date: 2025-10-03
categories: [AI Security, Machine Learning]
tags: [AI, Security, Adversarial ML, Cybersecurity]
---

ğŸ” #3: Evasion Attacks â€” Fooling Deployed AI Models
In the third post of my 7-part series on securing AI systems, I dive into evasion attacks â€” how attackers manipulate inputs after deployment to bypass AI models and what organizations can do to defend against them.

ğŸ¯ What Are Evasion Attacks?
Evasion attacks occur during the inference stage â€” when an AI model is making predictions.
 The adversary crafts perturbed inputs designed to trick the model into misclassifying data, even though the changes are imperceptible to humans.
These attacks can cause:
Financial losses â†’ e.g., bypassing fraud detection


Reputation damage â†’ manipulated sentiment analysis


Safety risks â†’ evading medical diagnostics or autonomous systems



ğŸ•µï¸ How Attackers Prepare
Before launching an evasion attack, adversaries often perform reconnaissance by:
Reviewing model cards & documentation


Using social engineering to gather details


Conducting dry-run probes against inference APIs


Leveraging transfer learning vulnerabilities from open-source base models



âš¡ Common Evasion Techniques
1ï¸âƒ£ Fast Gradient Sign Method (FGSM)
Generates one-step perturbations by adjusting each pixel in the direction that maximizes classification error.


Supported by Adversarial Robustness Toolkit (ART) via the FastGradientMethod class.


Uses an epsilon value to control perturbation strength.


2ï¸âƒ£ Projected Gradient Descent (PGD)
An iterative version of FGSM: applies multiple, smaller updates for refined perturbations.


Implemented in ARTâ€™s ProjectedGradientDescent module.


3ï¸âƒ£ Carlini & Wagner (C&W) Attack
Finds the smallest possible perturbation to cause targeted misclassification.


More stealthy than FGSM/PGD since it optimizes imperceptible changes.


4ï¸âƒ£ NLP Evasion Attacks (Text-based models)
Uses frameworks like TextAttack and BERT to manipulate input text:


Example: Changing a negative review into a positive one without altering its semantic meaning.


Can also mislead Natural Language Inference (NLI) models by replacing words strategically.



ğŸ›¡ï¸ Defending Against Evasion Attacks
Adversarial Training â†’ Retrain models with perturbed examples to improve robustness.


Input Preprocessing â†’ Apply transformations like image cropping, rotation, and scaling to neutralize malicious perturbations.


Gradient Masking â†’ Obscure gradient information by introducing Gaussian noise during training, making gradient-based attacks less effective.


Continuous Testing â†’ Use tools like:


Adversarial Robustness Toolkit (ART) â†’ Generate adversarial test cases.


TextAttack â†’ Test NLP models for evasion vulnerabilities.



Bottom line:
 Evasion attacks are one of the most common AI threats today â€” especially against deployed models and inference APIs. By combining adversarial testing, robust training, and secure MLOps practices, organizations can significantly reduce risk.

ğŸ’¬ Over to you:
 Have you tested your models against evasion attacks yet? What frameworks or strategies have worked best in your environment?
#AISecurity #EvasionAttacks #MLOps #AdversarialML #Cybersecurity #MachineLearning



